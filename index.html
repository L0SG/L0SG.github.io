<!DOCTYPE HTML>
<html lang="en">
<head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Sang-gil Lee</title>

  <meta name="author" content="Sang-gil Lee">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <link rel="stylesheet" type="text/css" href="stylesheet.css">
  <link rel="icon"
        href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>üåê</text></svg>">
</head>

<body>
<table
  style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
  <tbody>
  <tr style="padding:0px">
    <td style="padding:0px">
      <table
        style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
        <tbody>
        <tr style="padding:0px">
          <td style="padding:2.5%; vertical-align:middle; text-align:center;">
            <p style="text-align:center">
              <name>Sang-gil Lee</name>
            </p>
            <p style="text-align:center">
              <a href="mailto:tkdrlf9202@gmail.com">Email</a> &nbsp/&nbsp
              <a href="data/Sang_gil_Lee_CV.pdf">CV</a> &nbsp/&nbsp
              <a href="https://www.linkedin.com/in/sang-gil-lee/">LinkedIn</a> &nbsp/&nbsp
              <a href="https://scholar.google.com/citations?user=P93s2UQAAAAJ&hl=en">Google Scholar</a>
              &nbsp/&nbsp
              <a href="https://twitter.com/L0SG">X (Twitter)</a> &nbsp/&nbsp
              <a href="https://github.com/L0SG/">GitHub</a>
            </p>
          </td>
          <td style="padding:2.5%; vertical-align:middle; text-align:center;">
            <div class="item" style="text-align:center;">
              <div class="item2">
                <img src='images/L0SG.png'>
              </div>
              <img src='images/Sang-gil-Lee.png'>
            </div>
          </td>
        </tr>
        <tr style="padding:0px">
          <td style="padding:2.5%; vertical-align:middle" colspan="2">
            <p>
              I am a research scientist at <a href="https://www.nvidia.com/">NVIDIA</a>.
              I work on deep generative models for sequences, with a particular focus on speech and audio.
            </p>
            <p>
              I received my Ph.D. from the Data Science & AI Lab (DSAIL) at <a
              href="https://en.snu.ac.kr">Seoul National University</a>.
              During my Ph.D., I served as a research intern at <a href="https://www.nvidia.com/">NVIDIA</a>, under the advisement of <a
              href="https://wpingnet.github.io/">Wei Ping</a> and <a
              href="https://scholar.google.com/citations?user=7BRYaGcAAAAJ">Boris Ginsburg</a>.
              Prior to that, I completed internships at <a
              href="https://www.microsoft.com/en-us/research/lab/microsoft-research-asia/">Microsoft
              Research Asia</a>, where I was advised by <a href="https://tan-xu.github.io/">Xu Tan</a>, <a
              href="https://www.microsoft.com/en-us/research/people/taoqin/">Tao Qin</a> (speech), and
              <a href="https://www.binshao.info/">Bin Shao</a> (bioinformatics).
              I received my B.S. in Electrical and Computer Engineering from <a
              href="https://en.snu.ac.kr">Seoul National University</a>.
            </p>
          </td>
        </tbody>
      </table>

      <br>
      <br>

      <table
        style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
        <tbody>
        <tr>
          <td style="padding:20px;width:100%;vertical-align:middle">
            <heading>Research</heading>
            <p>
              My research interest spans a wide range of deep generative models (AR, flow, GAN, diffusion,
              etc.) applied to sequential data. Specifically, I am working on building multi-modal large language models
              with a focus on audio.<br><br>
              During my Ph.D., I focused on time-domain waveform data (speech and audio) to advance generative modeling for audio.<br><br>
              I am also broadly interested in speech and audio applications, including text-to-speech, voice conversion, music generation, neural audio codecs, and audio language models.<br><br>
              Representative papers are <span class="highlight">highlighted</span>.
            </p>
          </td>
        </tr>
        </tbody>
      </table>
      <table
        style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
        <tbody>
          
          <tr onmouseout="etta_stop()" onmouseover="etta_start()" bgcolor="#ffffd0">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <img src='images/etta.png' width="160">
              </div>
              <script type="text/javascript">
                  function etta_start() {
                      document.getElementById('etta_image').style.opacity = "1";
                  }
  
                  function etta_stop() {
                      document.getElementById('etta_image').style.opacity = "0";
                  }
  
                  etta_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/abs/2412.19351">
                <papertitle>ETTA: Elucidating the Design Space of Text-to-Audio Models</papertitle>
              </a>
              <br>
              <strong>Sang-gil Lee*</strong>,
              <a href="https://cseweb.ucsd.edu/~z4kong/">Zhifeng Kong*</a>,
              <a href="https://goelarushi.github.io">Arushi Goel</a>,
              <a href="https://scholar.google.com/citations?user=6qGppvkAAAAJ">Sungwon Kim</a>,
              <a href="https://rafaelvalle.github.io">Rafael Valle</a>,
              <a href="https://scholar.google.com/citations?user=UZ6kI2AAAAAJ">Bryan Catanzaro</a>
              <br>
              <em>arXiv preprint</em>, 2024
              <br> <a href="https://research.nvidia.com/labs/adlr/ETTA/">Project Page</a> /
              <a href="https://arxiv.org/abs/2412.19351">arXiv</a>
              <p></p>
              <p>ETTA is the first text-to-audio model with emergent abilities, capable of synthesizing entirely novel, imaginative sounds beyond the real world by leveraging large-scale synthetic audio captions (AF-Synthetic).</p>
            </td>
          </tr>

          <tr onmouseout="bigvgan_stop()" onmouseover="bigvgan_start()" bgcolor="#ffffd0">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <img src='images/bigvgan.png' width="160">
              </div>
              <script type="text/javascript">
                  function bigvgan_start() {
                      document.getElementById('bigvgan_image').style.opacity = "1";
                  }
  
                  function bigvgan_stop() {
                      document.getElementById('bigvgan_image').style.opacity = "0";
                  }
  
                  bigvgan_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/abs/2206.04658">
                <papertitle>BigVGAN: A Universal Neural Vocoder with Large-Scale Training</papertitle>
              </a>
              <br>
              <strong>Sang-gil Lee</strong>,
              <a href="https://wpingnet.github.io/">Wei Ping</a>,
              <a href="https://scholar.google.com/citations?user=7BRYaGcAAAAJ">Boris Ginsburg</a>,
              <a href="https://scholar.google.com/citations?user=UZ6kI2AAAAAJ/">Bryan Catanzaro</a>,
              <a href="https://scholar.google.com/citations?user=Bphl_fIAAAAJ">Sungroh Yoon</a>
              <br>
              <em>International Conference on Learning Representations (ICLR)</em>, 2023
              <br> <a href="https://research.nvidia.com/labs/adlr/projects/bigvgan/">Project Page</a> /
              <a href="https://huggingface.co/collections/nvidia/bigvgan-66959df3d97fd7d98d97dc9a">Model </a> /
              <a href="https://arxiv.org/abs/2206.04658">arXiv</a> /
              <a href="https://github.com/NVIDIA/BigVGAN">Code</a> /
              <a href="https://bigvgan-demo.github.io/">Demo</a>
              <p></p>
              <p>BigVGAN is a universal audio synthesizer that achieves unprecedented zero-shot performance on various
                unseen
                environments using anti-aliased periodic nonlinearity and large-scale training. </p>
            </td>
          </tr>

          <tr onmouseout="lfsc_stop()" onmouseover="lfsc_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <img src='images/lfsc.png' width="160">
              </div>
              <script type="text/javascript">
                  function lfsc_start() {
                      document.getElementById('lfsc_image').style.opacity = "1";
                  }
  
                  function lfsc_stop() {
                      document.getElementById('lfsc_image').style.opacity = "0";
                  }
  
                  lfsc_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/abs/2409.12117">
                <papertitle>Low Frame-rate Speech Codec: a Codec Designed for Fast High-quality Speech LLM Training and Inference</papertitle>
              </a>
              <br>
              <a href="https://scholar.google.com/citations?user=QvS6LVwAAAAJ">Edresson Casanova</a>,
              <a href="https://www.linkedin.com/in/ryan-langman-49401a4b/">Ryan Langman</a>,
              <a href="https://paarthneekhara.github.io">Paarth Neekhara</a>,
              <a href="https://shehzeen.github.io">Shehzeen Hussain</a>,
              <a href="https://scholar.google.com/citations?user=V28bxDwAAAAJ">Jason Li</a>,
              <a href="https://scholar.google.com/citations?user=HHn14y8AAAAJ">Subhankar Ghosh</a>,
              <a href="https://scholar.google.com/citations?user=ZleK6ccAAAAJ">Ante Jukiƒá</a>,
              <strong>Sang-gil Lee</strong>
              <br>
              <em>ICASSP</em>, 2025
              <br> <a href="https://huggingface.co/nvidia/low-frame-rate-speech-codec-22khz">Model</a> /
              <a href="https://arxiv.org/abs/2409.12117">arXiv</a>
              <p></p>
              <p>A neural audio codec that leverages finite scalar quantization and adversarial training with large speech language models to achieve high-quality audio compression with a 1.89 kbps bitrate and 21.5 frames per second.</p>
            </td>
          </tr>

          <tr onmouseout="synthetic_stop()" onmouseover="synthetic_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <img src='images/tangoaf.png' width="160">
              </div>
              <script type="text/javascript">
                  function synthetic_start() {
                      document.getElementById('synthetic_image').style.opacity = "1";
                  }
  
                  function synthetic_stop() {
                      document.getElementById('synthetic_image').style.opacity = "0";
                  }
  
                  synthetic_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/abs/2406.15487">
                <papertitle>Improving Text-To-Audio Models with Synthetic Captions</papertitle>
              </a>
              <br>
              <a href="https://cseweb.ucsd.edu/~z4kong/">Zhifeng Kong*</a>,
              <strong>Sang-gil Lee*</strong>,
              <a href="https://deepanwayx.github.io">Deepanway Ghosal</a>,
              <a href="https://scholar.google.com.sg/citations?user=jPfEvuQAAAAJ">Navonil Majumder</a>,
              <a href="https://scholar.google.com.sg/citations?user=4q8VxIIAAAAJ">Ambuj Mehrish</a>,
              <a href="https://rafaelvalle.github.io">Rafael Valle</a>,
              <a href="https://soujanyaporia.github.io">Soujanya Poria</a>,
              <a href="https://scholar.google.com/citations?user=UZ6kI2AAAAAJ">Bryan Catanzaro</a>
              <br>
              <em>Interspeech SynData4GenAI</em>, 2024
              <br> <a href="https://github.com/NVIDIA/audio-flamingo/tree/main/labeling_machine">Dataset</a> /
              <a href="https://huggingface.co/declare-lab/tango-af-ac-ft-ac">Model</a> /
              <a href="https://arxiv.org/abs/2406.15487">arXiv</a>
              <p></p>
              <p><a href="https://github.com/NVIDIA/audio-flamingo/tree/main/labeling_machine">AF-AudioSet</a> is a large-scale audio dataset with synthetic captions from <a href="https://audioflamingo.github.io/">Audio Flamingo</a> that leads to significant improvements to text-to-audio models.</p>
            </td>
          </tr>

          <tr onmouseout="voicetailor_stop()" onmouseover="voicetailor_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <img src='images/voicetailor.png' width="160">
              </div>
              <script type="text/javascript">
                  function voicetailor_start() {
                      document.getElementById('voicetailor_image').style.opacity = "1";
                  }
  
                  function voicetailor_stop() {
                      document.getElementById('voicetailor_image').style.opacity = "0";
                  }
  
                  voicetailor_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/abs/2408.14739">
                <papertitle>VoiceTailor: Lightweight Plug-In Adapter for Diffusion-Based Personalized Text-to-Speech</papertitle>
              </a>
              <br>
              <a href="https://gmltmd789.github.io/">Heeseung Kim</a>,
              <strong>Sang-gil Lee</strong>,
              <a href="https://scholar.google.com/citations?user=gxNOJPEAAAAJ&hl=en">Jiheum Yeom</a>,      
              <a href="https://scholar.google.com/citations?user=DDI2oS8AAAAJ&hl=en">Che Hyun Lee</a>,
              <a href="https://scholar.google.com/citations?user=6qGppvkAAAAJ&hl=en">Sungwon Kim</a>,
              <a href="https://scholar.google.com/citations?user=Bphl_fIAAAAJ&hl=en">Sungroh Yoon</a>
              <br>
              <em>INTERSPEECH</em>, 2024
              <br> <a href="https://voicetailor.github.io/">Project Page</a> /
              <a href="https://arxiv.org/abs/2408.14739">arXiv</a>
              <p></p>
              <p>VoiceTailor is a one-shot speaker-adaptive text-to-speech model, which proposes combining low-rank adapters to perform speaker adaptation in a parameter-efficient manner.</p>
            </td>
          </tr>     

        <tr onmouseout="editavideo_stop()" onmouseover="editavideo_start()">
          <td style="padding:20px;width:25%;vertical-align:middle">
            <div class="one">
              <img src='images/editavideo.gif' width="160">
            </div>
            <script type="text/javascript">
                function editavideo_start() {
                    document.getElementById('editavideo_image').style.opacity = "1";
                }

                function editavideo_stop() {
                    document.getElementById('editavideo_image').style.opacity = "0";
                }

                editavideo_stop()
            </script>
          </td>
          <td style="padding:20px;width:75%;vertical-align:middle">
            <a href="https://arxiv.org/abs/2303.07945">
              <papertitle>Edit-A-Video: Single Video Editing with Object-Aware Consistency</papertitle>
            </a>
            <br>
            <a href="https://scholar.google.com/citations?user=M8RX0MEAAAAJ">Chaehun Shin*</a>,
            <a href="https://gmltmd789.github.io">Heeseung Kim*</a>,
            Che Hyun Lee,
            <strong>Sang-gil Lee</strong>,
            <a href="https://scholar.google.com/citations?user=Bphl_fIAAAAJ">Sungroh Yoon</a>
            <br>
            <em>Asian Conference on Machine Learning (ACML)</em>, <strong>Best Paper Award</strong>, 2023
            <br> <a href="https://edit-a-video.github.io/">Project Page</a> /
            <a href="https://arxiv.org/abs/2303.07945">arXiv</a>
            <p></p>
            <p>Edit-A-Video is a diffusion-based one-shot video editing model that solves a background inconsistency problem using a new sparse-causal mask blending method. </p>
          </td>
        </tr>

        <tr onmouseout="priorgrad_stop()" onmouseover="priorgrad_start()">
          <td style="padding:20px;width:25%;vertical-align:middle">
            <div class="one">
              <img src='images/priorgrad.png' width="160">
            </div>
            <script type="text/javascript">
                function priorgrad_start() {
                    document.getElementById('priorgrad_image').style.opacity = "1";
                }

                function priorgrad_stop() {
                    document.getElementById('priorgrad_image').style.opacity = "0";
                }

                priorgrad_stop()
            </script>
          </td>
          <td style="padding:20px;width:75%;vertical-align:middle">
            <a href="https://openreview.net/pdf?id=_BNiN4IjC5">
              <papertitle>PriorGrad: Improving Conditional Denoising Diffusion Models with Data-Dependent Adaptive
                Prior
              </papertitle>
            </a>
            <br>
            <strong>Sang-gil Lee</strong>,
            <a href="https://gmltmd789.github.io">Heeseung Kim</a>,
            <a href="https://scholar.google.com/citations?user=M8RX0MEAAAAJ">Chaehun Shin</a>,
            <a href="https://tan-xu.github.io/">Xu Tan</a>,
            <a href="https://changliu00.github.io/">Chang Liu</a>,
            <a href="https://www.microsoft.com/en-us/research/people/meq/">Qi Meng</a>,
            <a href="https://www.microsoft.com/en-us/research/people/taoqin/">Tao Qin</a>,
            <a href="https://weichen-cas.github.io/">Wei Chen</a>,
            <a href="https://scholar.google.com/citations?user=Bphl_fIAAAAJ">Sungroh Yoon</a>,
            <a href="https://www.microsoft.com/en-us/research/people/tyliu/">Tie-Yan Liu</a>
            <br>
            <em>International Conference on Learning Representations (ICLR)</em>, 2022
            <br> <a href="https://speechresearch.github.io/priorgrad/">Project Page</a> /
            <a href="https://arxiv.org/abs/2106.06406">arXiv</a> /
            <a href="https://github.com/microsoft/NeuralSpeech">Code</a> /
            <a href="https://iclr.cc/virtual/2022/poster/6445">Poster</a>
            <p></p>
            <p>PriorGrad presents an efficient method for constructing a data-dependent non-standard Gaussian prior for
              training and sampling from diffusion models applied to speech synthesis. </p>
          </td>
        </tr>

        <tr onmouseout="nanoflow_stop()" onmouseover="nanoflow_start()">
          <td style="padding:20px;width:25%;vertical-align:middle">
            <div class="one">
              <img src='images/nanoflow.png' width="160">
            </div>
            <script type="text/javascript">
                function nanoflow_start() {
                    document.getElementById('nanoflow_image').style.opacity = "1";
                }

                function nanoflow_stop() {
                    document.getElementById('nanoflow_image').style.opacity = "0";
                }

                nanoflow_stop()
            </script>
          </td>
          <td style="padding:20px;width:75%;vertical-align:middle">
            <a href="https://proceedings.neurips.cc/paper/2020/file/a1c3ae6c49a89d92aef2d423dadb477f-Paper.pdf">
              <papertitle>NanoFlow: Scalable Normalizing Flows with Sublinear Parameter Complexity</papertitle>
            </a>
            <br>
            <strong>Sang-gil Lee</strong>,
            <a href="https://scholar.google.com/citations?user=6qGppvkAAAAJ">Sungwon Kim</a>,
            <a href="https://scholar.google.com/citations?user=Bphl_fIAAAAJ">Sungroh Yoon</a>
            <br>
            <em>Neural Information Processing Systems (NeurIPS)</em>, 2020
            <br>
            <a href="https://arxiv.org/abs/2006.06280">arXiv</a> /
            <a href="https://github.com/L0SG/NanoFlow">Code</a> /
            <a href="https://nips.cc/virtual/2020/poster/17696">Poster</a>
            <p></p>
            <p>NanoFlow uses a single neural network for multiple transformation stages in normalizing flows, which
              provides an efficient compression for flow-based generative models.</p>
          </td>
        </tr>

        <tr onmouseout="flowavenet_stop()" onmouseover="flowavenet_start()">
          <td style="padding:20px;width:25%;vertical-align:middle">
            <div class="one">
              <img src='images/flowavenet.png' width="160">
            </div>
            <script type="text/javascript">
                function flowavenet_start() {
                    document.getElementById('flowavenet_image').style.opacity = "1";
                }

                function flowavenet_stop() {
                    document.getElementById('flowavenet_image').style.opacity = "0";
                }

                flowavenet_stop()
            </script>
          </td>
          <td style="padding:20px;width:75%;vertical-align:middle">
            <a href="http://proceedings.mlr.press/v97/kim19b/kim19b.pdf">
              <papertitle>FloWaveNet: A Generative Flow for Raw Audio</papertitle>
            </a>
            <br>
            <a href="https://scholar.google.com/citations?user=6qGppvkAAAAJ">Sungwon Kim</a>,
            <strong>Sang-gil Lee</strong>,
            <a href="https://scholar.google.com/citations?user=AcVToQUAAAAJ">Jongyoon Song</a>,
            <a href="https://jaywalnut310.github.io/">Jaehyeon Kim</a>,
            <a href="https://scholar.google.com/citations?user=Bphl_fIAAAAJ">Sungroh Yoon</a>
            <br>
            <em>International Conference on Machine Learning (ICML)</em>, 2019
            <br>
            <a href="https://arxiv.org/abs/1811.02155">arXiv</a> /
            <a href="https://github.com/ksw0306/FloWaveNet">Code</a> /
            <a href="https://ksw0306.github.io/flowavenet-demo">Demo</a> /
            <a href="https://pdfs.semanticscholar.org/9e79/377defb3385ae4dfb5e345c85686e27ca7a5.pdf">Poster</a>
            <p></p>
            <p>FloWaveNet is one of the first flow-based generative models for fast and parallel synthesis of audio waveforms, enabling a likelihood-based neural vocoder without any auxiliary loss.</p>
          </td>
        </tr>

        <tr onmouseout="ttsql_stop()" onmouseover="ttsql_start()">
          <td style="padding:20px;width:25%;vertical-align:middle">
            <div class="one">
              <img src='images/ttsql.png' width="160">
            </div>
            <script type="text/javascript">
                function ttsql_start() {
                    document.getElementById('ttsql_image').style.opacity = "1";
                }

                function ttsql_stop() {
                    document.getElementById('ttsql_image').style.opacity = "0";
                }

                ttsql_stop()
            </script>
          </td>
          <td style="padding:20px;width:75%;vertical-align:middle">
            <a href="https://arxiv.org/abs/1905.11499">
              <papertitle>One-Shot Learning for Text-to-SQL Generation</papertitle>
            </a>
            <br>
            <a href="https://scholar.google.com/citations?user=5Psi6aYAAAAJ">Dongjun Lee</a>,
            <a href="https://www.jaesikyoon.com/">Jaesik Yoon</a>,
            <a href="https://scholar.google.com/citations?user=AcVToQUAAAAJ">Jongyoon Song</a>,
            <strong>Sang-gil Lee</strong>,
            <a href="https://scholar.google.com/citations?user=Bphl_fIAAAAJ">Sungroh Yoon</a>
            <br>
            <em>arXiv preprint</em>, 2019
            <br>
            <a href="https://arxiv.org/abs/1905.11499">arXiv</a>
            <p></p>
            <p>Template-based one-shot text-to-SQL generative model based on a Candidate Search Network & Pointer
              Network.</p>
          </td>
        </tr>

        <tr onmouseout="seqgan_stop()" onmouseover="seqgan_start()">
          <td style="padding:20px;width:25%;vertical-align:middle">
            <div class="one">
              <img src='images/seqgan.png' width="160">
            </div>
            <script type="text/javascript">
                function seqgan_start() {
                    document.getElementById('seqgan_image').style.opacity = "1";
                }

                function seqgan_stop() {
                    document.getElementById('seqgan_image').style.opacity = "0";
                }

                seqgan_stop()
            </script>
          </td>
          <td style="padding:20px;width:75%;vertical-align:middle">
            <a href="https://arxiv.org/abs/1710.11418">
              <papertitle>Polyphonic Music Generation with Sequence Generative Adversarial Networks
              </papertitle>
            </a>
            <br>
            <strong>Sang-gil Lee</strong>,
            <a href="https://sites.google.com/view/uiwon-hwang">Uiwon Hwang</a>,
            <a href="https://scholar.google.co.kr/citations?user=dWKk68wAAAAJ">Seonwoo Min</a>,
            <a href="https://scholar.google.com/citations?user=Bphl_fIAAAAJ">Sungroh Yoon</a>
            <br>
            <em>arXiv preprint</em>, 2017
            <br>
            <a href="https://arxiv.org/abs/1710.11418">arXiv</a> /
            <a href="https://github.com/L0SG/seqgan-music">Code</a>
            <p></p>
            <p>This work investigates an efficient musical word representation from polyphonic MIDI data for SeqGAN, simultaneously capturing chords and melodies with dynamic timings.</p>
          </td>
        </tr>

        <tr onmouseout="snn_stop()" onmouseover="snn_start()">
          <td style="padding:20px;width:25%;vertical-align:middle">
            <div class="one">
              <img src='images/snn.png' width="160">
            </div>
            <script type="text/javascript">
                function snn_start() {
                    document.getElementById('snn_image').style.opacity = "1";
                }

                function snn_stop() {
                    document.getElementById('snn_image').style.opacity = "0";
                }

                snn_stop()
            </script>
          </td>
          <td style="padding:20px;width:75%;vertical-align:middle">
            <a href="https://arxiv.org/abs/1611.02416">
              <papertitle>An Efficient Approach to Boosting Performance of Deep Spiking Network Training
              </papertitle>
            </a>
            <br>
            Seongsik Park,
            <strong>Sang-gil Lee</strong>,
            Hyunha Nam,
            <a href="https://scholar.google.com/citations?user=Bphl_fIAAAAJ">Sungroh Yoon</a>
            <br>
            <em>Neural Information Processing Systems (NIPS) Workshop on Computing with Spikes</em>, 2016
            <br>
            <a href="https://arxiv.org/abs/1611.02416">arXiv</a>
            <p></p>
            <p>Investigates various initialization and backward control schemes of the membrane potential for training
              deep spiking networks.</p>
          </td>
        </tr>


        </tbody>
      </table>

      <br>
      <br>

      <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
        <tbody>
        <tr>
          <td style="padding:20px;width:100%;vertical-align:middle">
            <heading>Experience</heading>
          </td>
        </tr>
        </tbody>
      </table>

      <table width="100%" align="center" border="0" cellpadding="20">
        <tbody>

        <table width="100%" align="center" border="0" cellpadding="20">
          <tbody>

          <tr>
            <td style="padding:10px;width:25%;vertical-align:middle">
              <div class="one">
                <img src='images/nvidia.png' width="160">
              </div>
            </td>
            <td style="padding:10px;width:75%;vertical-align:top">

                          <papertitle><span class="bigger">
                    Research Scientist @ NVIDIA
                    </span></papertitle>
              <br>
              Jan 2024 - Current
              <br>
              In the <a href="https://research.nvidia.com/labs/adlr/ ">Applied Deep Learning Research</a> team, I am working on building multi-modal large language models with a focus on audio.
              <br>
              Sep 2021 - Jan 2022
              <br>
              As a research intern, I worked on improving neural vocoders for high quality speech and audio synthesis, advised by
              <a href="https://wpingnet.github.io/">Wei Ping</a> and
              <a href="https://scholar.google.com/citations?user=7BRYaGcAAAAJ">Boris Ginsburg</a>.

            </td>
          </tr>

          <tr>
          <td style="padding:10px;width:25%;vertical-align:middle">
            <div class="one">
              <img src='images/qcai.jpg' width="180">
            </div>
          </td>
          <td style="padding:10px;width:75%;vertical-align:top">

            <papertitle><span class="bigger">
                    Senior Research Engineer @ Qualcomm AI Research
                    </span></papertitle>
            <br>
            Feb 2023 - Jan 2024
            <br>
            <br>
            I developed a framework for Text-to-Speech (TTS) research and development, optimized for deployment on edge devices.

          </td>
        </tr>

          <tr>
            <td style="padding:10px;width:25%;vertical-align:middle">
              <div class="one">
                <img src='images/microsoft.jpeg' width="140">
              </div>
            </td>
            <td style="padding:10px;width:75%;vertical-align:top">

              <papertitle><span class="bigger">
                    Research Intern @ Microsoft Research Asia
                    </span></papertitle>
              <br>
              Dec 2020 - May 2021
              <br>
              I worked on diffusion-based generative models for speech synthesis, advised by
              <a href="https://tan-xu.github.io/">Xu Tan</a>,
              <a href="https://changliu00.github.io/">Chang Liu</a>,
              <a href="https://www.microsoft.com/en-us/research/people/meq/">Qi Meng</a>, and
              <a href="https://www.microsoft.com/en-us/research/people/taoqin/">Tao Qin</a>.
              <br>
              Dec 2018 - Feb 2019
              <br>
              I worked on <a href="https://www.microsoft.com/en-us/research/project/immunomics/">the Antigen Map
              Project</a>,
              where I applied sequence models to predict antigens from genetic sequences, advised by
              <a href="https://www.binshao.info/">Bin Shao</a>.

            </td>
          </tr>

          <tr>
            <td style="padding:10px;width:25%;vertical-align:middle">
              <div class="one">
                <img src='images/kakao.png' width="140">
              </div>
            </td>
            <td style="padding:10px;width:75%;vertical-align:top">

              <papertitle><span class="bigger">
                    Research Intern @ Kakao Corporation
                    </span></papertitle>

              <br>
              Jul 2019 - Sep 2019
              <br>
              <br>
              I worked on improving speech synthesis and voice conversion models, advised by
              <a href="https://jaywalnut310.github.io/">Jaehyeon Kim</a> and <a
              href="https://scholar.google.com/citations?user=xUMMrvwAAAAJ">Jaekyong Bae</a>.

            </td>
          </tr>


          </tbody>
        </table>

        <br>
        <br>

        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
          <tbody>
          <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Education</heading>
            </td>
          </tr>
          </tbody>
        </table>

        <table width="100%" align="center" border="0" cellpadding="20">
          <tbody>

          <tr>
            <td style="padding:10px;width:25%;vertical-align:middle">
              <div class="one">
                <img src='images/snu.png' width="140">
              </div>
            </td>
            <td style="padding:10px;width:75%;vertical-align:top">

              <papertitle><span class="bigger">
                    Ph.D. in Seoul National University
                    </span></papertitle>
              <br>
              <span class="bigger">
                    Electrical and Computer Engineering
                    </span>
              <br>
              Sep 2016 - Feb 2023
              <li>Dissertation: <a href="https://snu-primo.hosted.exlibrisgroup.com/permalink/f/1l6eo7m/82SNU_INST51890946960002591">Deep Generative Model for Waveform Synthesis</a></li>
              <li>Integrated M.S./Ph.D. Program. &nbsp; Advisor: <a
                href="https://scholar.google.com/citations?user=Bphl_fIAAAAJ">Sungroh Yoon</a>.</li>
              <br>
              <papertitle><span class="bigger">
                    Dual B.S. in Seoul National University
                    </span></papertitle>
              <br>
              <span class="bigger">
                    Electrical and Computer Engineering / Applied Biology and Chemistry
                    </span>
              <br>
              Mar 2010 - Aug 2016
              <br>
              <li>Cum Laude</li>
            </td>
          </tr>

          </tbody>
        </table>

        <br>
        <br>

        <table
          style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
          <tbody>
          <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Projects</heading>
              <p>
                During my time at DSAIL, I collaborated with <a href="http://www.snuh.org/global/en/main.do">Seoul
                National University Hospital</a> on a computer-aided diagnosis project for liver cancer.
                The project yielded a high-performance medical object detection model to help reduce human errors from radiologists for the early detection of liver disease.
              </p>
            </td>
          </tr>
          </tbody>
        </table>
        <table
          style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
          <tbody>

          <tr onmouseout="gssdpp_stop()" onmouseover="gssdpp_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <img src='images/gssdpp.png' width="160">
              </div>
              <script type="text/javascript">
                  function gssdpp_start() {
                      document.getElementById('gssdpp_image').style.opacity = "1";
                  }

                  function gssdpp_stop() {
                      document.getElementById('gssdpp_image').style.opacity = "0";
                  }

                  gssdpp_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://ieeexplore.ieee.org/abstract/document/9650690/">
                <papertitle>Robust End-to-End Focal Liver Lesion Detection Using Unregistered Multiphase Computed
                  Tomography Images
                </papertitle>
              </a>
              <br>
              <strong>Sang-gil Lee*</strong>,
              <a href="https://sites.google.com/snu.ac.kr/eunjikim">Eunji Kim*</a>,
              <a href="https://scholar.google.co.kr/citations?user=C_GxLiAAAAAJ">Jae Seok Bae*</a>,
              Jung Hoon Kim,
              <a href="https://scholar.google.com/citations?user=Bphl_fIAAAAJ">Sungroh Yoon</a>
              <br>
              <em>IEEE Transactions on Emerging Topics in Computational Intelligence (TETCI)</em>, 2021
              <br>
              <a href="https://arxiv.org/abs/2112.01535">arXiv</a> /
              <a href="https://github.com/L0SG/grouped-ssd-pytorch">Code</a>
              <p></p>
              <p>GSSD++ provides robustness to unregistered multi-phase CT images for detecting liver lesions using
                attention-guided multi-phase alignment with deformable convolutions. </p>
            </td>
          </tr>

          <tr onmouseout="gssd_stop()" onmouseover="gssd_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <img src='images/gssd.png' width="160">
              </div>
              <script type="text/javascript">
                  function gssd_start() {
                      document.getElementById('gssd_image').style.opacity = "1";
                  }

                  function gssd_stop() {
                      document.getElementById('gssd_image').style.opacity = "0";
                  }

                  gssd_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://link.springer.com/chapter/10.1007/978-3-030-00934-2_77">
                <papertitle>Liver Lesion Detection from Weakly-Labeled Multi-phase CT Volumes with a Grouped Single Shot
                  MultiBox Detector
                </papertitle>
              </a>
              <br>
              <strong>Sang-gil Lee</strong>,
              <a href="https://scholar.google.co.kr/citations?user=C_GxLiAAAAAJ">Jae Seok Bae</a>,
              Hyunjae Kim,
              Jung Hoon Kim,
              <a href="https://scholar.google.com/citations?user=Bphl_fIAAAAJ">Sungroh Yoon</a>
              <br>
              <em>International Conference on Medical Image Computing and Computer-Assisted Intervention (MICCAI)</em>,
              2018
              <br>
              <a href="https://arxiv.org/abs/1807.00436">arXiv</a> /
              <a href="https://github.com/L0SG/grouped-ssd-pytorch">Code</a>
              <p></p>
              <p>GSSD pioneers a focal liver lesion detection model from multi-phase CT images, which reflects a
                real-world clinical practice of radiologists. </p>
            </td>
          </tr>

          </tbody>
        </table>

        <br>
        <br>

        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
          <tbody>
          <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Invited Talks, Honors, and Awards</heading>
            </td>
          </tr>
          </tbody>
        </table>

        <table border=0 class="bg_colour"
               style="padding:20px;width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
          <tbody>
          <ul>
            <li>Invited Talk "Deep Generative Model for Speech and Audio", <a href="https://ssu.ac.kr">Soongsil
              University</a>, 2023
            </li>
            <li>Invited Talk "Towards Universal Neural Waveform Synthesis", <a
              href="https://www.navercorp.com/en">Naver</a>, 2022
            </li>
            <li>Invited Talk "On Neural Waveform Synthesis", <a href="https://supertone.ai/">Supertone</a>, 2022</li>
            <li>Invited Talk "Prior Enhancement for Deep Generative Models", <a href="https://airsc.ai/">Hyundai
              AIRS</a>,
              2022
            </li>
            <li>Student Conference Scholarship, <a href="https://ai.google/">Google</a>, 2022</li>
            <li>Invited Talk "Neural Speech Synthesis: a 2021 Landscape", <a href="https://www.nvidia.com/">NVIDIA</a>,
              2021
            </li>
            <li>Graduate Student of the Year, DSAIL, <a href="https://en.snu.ac.kr">Seoul National University</a>, 2019
            </li>
            <li>Best Paper Award, <a href="https://airsc.ai/">Hyundai AIR Lab (currently AIRS)</a>, 2019</li>
            <li>Stars of Tomorrow (Excellent Intern), <a
              href="https://www.microsoft.com/en-us/research/lab/microsoft-research-asia/">Microsoft Research Asia</a>,
              2019
            </li>
            <li>Invited Talk "RNN Plus Alpha: Is RNN the False Prophet?", <a href="https://clova.ai">Naver CLOVA</a>,
              2018
            </li>
            <li>Cum Laude, Seoul National University, 2016</li>
            <li>Academic Performance Scholarship, <a href="https://en.snu.ac.kr">Seoul National University</a>, 2010 -
              2016
            </li>
            <li>Academic Scholarship (fully funded), <a href="https://foundation.sbs.co.kr/">SBS Foundation</a>, 2010 -
              2016
            </li>
          </ul>
          </tbody>
        </table>


        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
          <tbody>
          <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Personal</heading>
            </td>
          </tr>
          </tbody>
        </table>

        <table width="100%" align="center" border="0" cellpadding="20">
          <tbody>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle"><img src="images/L0SG_logo.jpg"
                                                                          width=140px></td>
            <td width="75%" valign="center">
              I am a PC hardware enthusiast, always eager to learn about computers in my free time.
              <br>
              <br>
              As a hobbyist DJ, I enjoy house music. <a
              href="https://www.youtube.com/channel/UC2z2cvrNrA2wJsHcIAMwGdw">My mixes on YouTube</a>
            </td>
          </tr>
          </tbody>
        </table>

        <table
          style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
          <tbody>
          <tr>
            <td style="padding:0px">
              <br>
              <p style="text-align:right;font-size:small;">
                Last update: Jan 2025. Template borrowed from <a
                href="https://github.com/jonbarron/jonbarron_website">here</a>.
              </p>
            </td>
          </tr>
          </tbody>
        </table>
    </td>
  </tr>
</table>
</body>

</html>
